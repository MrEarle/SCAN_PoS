{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech auxiliary task for SCAN\n",
    "\n",
    "Benjamin Earle, Jorge Pérez\n",
    "\n",
    "INF522 -- Text Mining\n",
    "\n",
    "19/12/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contexto\n",
    "\n",
    "Se trabaja sobre el problema presentado en el paper \"*Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks*\"\n",
    "\n",
    "> Si yo se lo que significa “jump”, “run”, “walk”, “run twice” y “walk twice”, debiera saber que significa “jump twice”\n",
    "\n",
    "<span style=\"text-decoration: underline\">Problema</span>: En el paper se muestra que las redes recurrentes fallan catastróficamente en esto, mientras que para nosotros es algo natural.\n",
    "\n",
    "Esto se mide en un dataset introducido en el paper llamado SCAN\n",
    "\n",
    "## Dataset: SCAN\n",
    "\n",
    "El dataset SCAN consiste en un problema de traducción de instrucciones en lenguaje natural a una secuencia de acciones.\n",
    "\n",
    "> jump left twice and run $\\longrightarrow$ TURN_LEFT JUMP TURN_LEFT JUMP RUN\n",
    "\n",
    "Para una red entrenada con ejemplos de `run twice`, `walk twice` y `jump` (pero nunca `jump twice`), no es capaz de generalizar para traducir bien `jump twice`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propuesta: Part of Speech\n",
    "\n",
    "<span style=\"text-decoration: underline\">Hipótesis (del paper)</span>: No se logra generalizar composicionalmente el uso de “jump” porque no se da suficiente evidencia de que funciona igual que “run” o “walk”.\n",
    "\n",
    "- Algo que comparte “jump”, “run” y “walk” es que todos son verbos, es decir, tienen el mismo Part of Speech\n",
    "- Se propone incluir información de PoS al entrenar y/o al momento de probar la red en el split de test\n",
    "\n",
    "<span style=\"text-decoration: underline\">Hipótesis (propia)</span>: La inclusión de PoS puede dar evidencia suficiente al modelo de que palabras con el mismo PoS se utilizan de la misma manera.\n",
    "\n",
    "Se probará con modelos LSTM, LSTM con atención y Transformer.\n",
    "\n",
    "## Como incluir Part of Speech\n",
    "\n",
    "### Tarea Auxiliar:\n",
    "\n",
    "![aux](assets/aux_model.png)\n",
    "\n",
    "- Se le pide a la red que prediga el PoS de cada palabra del input.\n",
    "\n",
    "- Se espera que lo aprendido por esta tarea auxiliar ayude a ejecutar la tarea principal.\n",
    "\n",
    "### Input Extra\n",
    "\n",
    "![aux](assets/input.png)\n",
    "\n",
    "- Se le entrega a la red el PoS como input.\n",
    "\n",
    "- Se espera que la red aprenda a usar esta información para ejecutar la tarea.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "Se probaron 3 splits distintos del dataset:\n",
    "\n",
    "- Simple: Los datos de train y test son i.i.d.\n",
    "- Addprim-Jump: El entrenamiento solo incluye `jump` sin modificadores (como `twice` o `around`), mientras que en test se busca medir si aprende a modificar `jump` (E.g. `jump twice` $\\rightarrow$ `JUMP JUMP`)\n",
    "- MCD1: Un split generado algoritmicamente, buscando una baja diferencia de átomos (distribución de palabras) y una gran divergencia de elementos compuestos.\n",
    "\n",
    "## LSTM (con y sin atención)\n",
    "\n",
    "Se usan los hiperparámetros del paper original:\n",
    "\n",
    "- Hidden Size: 100\n",
    "- \\# Layers: 1\n",
    "- Dropout: 0.1\n",
    "\n",
    "Se corre 3 veces cada experimento y se promediaron, obteniendo los siguientes resultados:\n",
    "\n",
    "Uso de PoS | Simple | Addprim Jump | MCD1\n",
    "----------:|:------:|:------------:|:---:\n",
    "Sin PoS    | 81.4%  | 58.8%        | 69.1%\n",
    "Input      | 81.4%  | 58.6%        | 68.4%\n",
    "Aux        | 81.4%  | **70%**      | **69.2%**\n",
    "\n",
    "![lstm_val_acc](assets/lstm_val_acc.png)\n",
    "\n",
    "- Se puede ver que para Simple y MCD1, el uso de PoS no influye en el resultado\n",
    "- Para el caso de Addprim-Jump, usar PoS como tarea auxiliar logra mejorar el resultado en más de 10%\n",
    "\n",
    "## Transformer\n",
    "\n",
    "Hiperparámetros utilizados:\n",
    "\n",
    "- Model Dim: 128\n",
    "- Heads: 4\n",
    "- Layers: 3\n",
    "- Dropout: 0.1\n",
    "\n",
    "Resultados:\n",
    "\n",
    "Uso de PoS | Simple | Addprim Jump | MCD1\n",
    "----------:|:------:|:------------:|:----:\n",
    "Sin PoS    | **77%**| **56%**      | 68%\n",
    "Aux        | 76%    | 51%          | 68%\n",
    "\n",
    "![transformer_acc](assets/transformer.png)\n",
    "\n",
    "- Se ponía a sobre-entrenar muy rápido.\n",
    "  - Al maximizar la tarea auxiliar, se comenzaba a sobre ajustar en la tarea principal\n",
    "  - Luego de ~10 épocas, las métricas en validación comenzaban a empeorar\n",
    "  - Todo esto, independiente del split\n",
    "- Resultados variaban harto según la inicialización\n",
    "- Nuestra hipótesis es que el modelo quedaba muy sobre-parametrizado\n",
    "\n",
    "### Ejemplos de resultados\n",
    "\n",
    "Algunos resultados del modelo LSTM entrenado en Addprim-Jump con la tarea auxiliar.\n",
    "\n",
    "`jump` $\\longrightarrow$ `WALK`\n",
    "\n",
    "`jump twice` $\\longrightarrow$ `WALK WALK`\n",
    "\n",
    "`jump left` $\\longrightarrow$ `TURN_LEFT WALK`\n",
    "\n",
    "`jump and run` $\\longrightarrow$ `RUN RUN`\n",
    "\n",
    "`jump around left and run opposite right` $\\longrightarrow$ `TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT WALK TURN_LEFT RUN TURN_RIGHT TURN_RIGHT RUN`\n",
    "\n",
    "Podemos ver claramente lo que el modelo aprendió. Efectivamente, aprender a predecir Part of Speech ayudó a que el modelo tenga evidencia de que `jump` se comporta igual que `walk` y `run`, lo que se nota al modificar correctamente el verbo. Pero aprendió tan bien eso, que ya no traduce `jump` a `JUMP`, sino que lo traduce a `WALK` o `RUN`, dependiendo del contexto.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusión\n",
    "\n",
    "Añadir la tarea auxiliar de Part of Speech ayuda a aprender la estructura sintáctica de la tarea, mejorando su rendimiento. Aún así, no mantiene la estructura semántica del problema, por lo que entrega respuestas mal traducidas para `jump`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "Aqui se pude ver el funcionamiento de la LSTM con atención entrenada en Addprim Jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.models.lstm import Seq2SeqAttentionLSTM, Seq2SeqLSTM\n",
    "from src.data.scan import (\n",
    "    MAX_SEQUENCE_LENGTH,\n",
    "    IN_VOCAB_FILE,\n",
    "    POS_VOCAB_SIZE,\n",
    "    OUT_VOCAB_SIZE,\n",
    "    IN_VOCAB_SIZE,\n",
    "    POS_VOCAB_FILE,\n",
    "    OUT_VOCAB_FILE,\n",
    ")\n",
    "from src.utils.constants import ACTION_OUTPUT_NAME, COMMAND_INPUT_NAME, POS_INPUT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_vectorizer = layers.TextVectorization(\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH, output_mode=\"int\", max_tokens=IN_VOCAB_SIZE, standardize=None\n",
    ")\n",
    "pos_vectorizer = layers.TextVectorization(\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH, output_mode=\"int\", max_tokens=POS_VOCAB_SIZE, standardize=None\n",
    ")\n",
    "out_vectorizer = layers.TextVectorization(\n",
    "    output_sequence_length=MAX_SEQUENCE_LENGTH, output_mode=\"int\", max_tokens=OUT_VOCAB_SIZE, standardize=None\n",
    ")\n",
    "\n",
    "\n",
    "in_vectorizer.set_vocabulary(IN_VOCAB_FILE)\n",
    "pos_vectorizer.set_vocabulary(POS_VOCAB_FILE)\n",
    "out_vectorizer.set_vocabulary(OUT_VOCAB_FILE)\n",
    "\n",
    "in_voc = in_vectorizer.get_vocabulary()\n",
    "out_voc = out_vectorizer.get_vocabulary()\n",
    "pos_voc = pos_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_in = \"<sos> jump and run <eos>\"\n",
    "model_pos = \"NOUN CONJ VERB\"\n",
    "\n",
    "model_in_vec = in_vectorizer(tf.convert_to_tensor([model_in, model_in]))\n",
    "model_pos_vec = pos_vectorizer(tf.convert_to_tensor([model_pos, model_pos]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x183b8860ee0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "hidden_layers = 1\n",
    "include_pos_tag = \"aux\"\n",
    "use_attention = True\n",
    "\n",
    "Model = Seq2SeqAttentionLSTM if use_attention else Seq2SeqLSTM\n",
    "\n",
    "model = Model(\n",
    "    hidden_size=hidden_size,\n",
    "    hidden_layers=hidden_layers,\n",
    "    include_pos_tag=include_pos_tag,\n",
    "    teacher_forcing=0,\n",
    ")\n",
    "\n",
    "pre_res = model({COMMAND_INPUT_NAME: model_in_vec, POS_INPUT_NAME: model_pos_vec}, training=False)\n",
    "checkpoint_path = f\"snap/addprim_jump-h_size({hidden_size})-h_layers({hidden_layers})-dropout(0.1){f'-pos({include_pos_tag})' if include_pos_tag else ''}{'-attention' if use_attention else ''}/best_action_accuracy\"\n",
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_LEFT I_WALK I_TURN_RIGHT I_TURN_RIGHT I_RUN\n"
     ]
    }
   ],
   "source": [
    "model_in = \"<sos> jump around left and run opposite right <eos>\"\n",
    "\n",
    "model_in_vec = in_vectorizer(tf.convert_to_tensor([model_in, model_in]))\n",
    "pre_res = model({COMMAND_INPUT_NAME: model_in_vec, POS_INPUT_NAME: model_pos_vec}, training=False)\n",
    "\n",
    "actions = tf.argmax(pre_res[ACTION_OUTPUT_NAME][0], axis=-1)\n",
    "actions = \" \".join([out_voc[i] for i in actions]).strip().split(\" \")\n",
    "actions = [a for a in actions if a not in [\"<sos>\", \"<eos>\"]]\n",
    "actions = \" \".join(actions)\n",
    "\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Demo en COLAB: [https://colab.research.google.com/drive/1RyAa8wZw0c156xODGotz6kd1AeA2SRLG?usp=sharing](https://colab.research.google.com/drive/1RyAa8wZw0c156xODGotz6kd1AeA2SRLG?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e693c1a5948535cd0a63fa6f245053b78120c9e0380f8ba709fae81d9dc1df8"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('tf_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
